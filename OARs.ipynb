{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Outdoor Adventure Recommendations (OARs)**\n",
        "\n",
        "<U>Problem Statement</U><br>\n",
        "Finding outdoor activities that match a user's fitness level, preferences, and location can be overwhelming due to the abundance of options and lack of personalized recommendations. Users often spend hours researching trails or activities, only to find that their chosen options don't align with their expectations or capabilities.<br>\n",
        "<U>Solution</U><br>\n",
        "This project aims to develop a recommendation system that provides personalized outdoor adventure suggestions. By analyzing trail features (e.g., difficulty, length, terrain) and user attributes (e.g., fitness level, activity preferences, equipment), the system will deliver tailored recommendations. This solution is valuable for outdoor enthusiasts seeking efficient, reliable, and personalized suggestions, enhancing their experiences and saving them time.<br>\n",
        "<U>Approach</U><br>\n",
        "\n",
        "\n",
        "1.   We will pull hiking trail data from the Overpass API. From this we will get features like length, elevation, whether dogs are allowed, etc.\n",
        "2.   Using and unsupervised algorithm, the trails will be grouped into clusters.\n",
        "3.   We will bootstrap our program by simulating user data and, using business knowledge, we will assign our simulated users to the trail clusters.\n",
        "4.   A supervised model will be trained on the user data.\n",
        "5.   The trained model will be applied to new users so that the system can provide them their recommended trails.<br>\n",
        "At the end, we will explore future enhancements.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UEozHI1gmicA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 484,
      "metadata": {
        "id": "LLA8005xvbvn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#installing all required packages\n",
        "import umap\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import adjusted_rand_score, silhouette_score, accuracy_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colormaps\n",
        "from matplotlib.colors import ListedColormap\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import requests\n",
        "import time\n",
        "from geopy.distance import geodesic\n",
        "import random\n",
        "from faker import Faker\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore') #turning off warnings to mak the final output more clear"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will define some functions for processing the data. This makes things a bit cleaner and will also help with reuseability in the future as we expand our program into other outdoor activities."
      ],
      "metadata": {
        "id": "VmTLHYUL0rSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this function is used to data for specific geographic locations from the Overpass API, including latitude, longitude, and elevation\n",
        "def fetch_node_coordinates(node_ids):\n",
        "    if not node_ids:\n",
        "        return []\n",
        "\n",
        "    node_ids_str = \",\".join(map(str, node_ids))  # Create a comma-separated list of node IDs\n",
        "    query = f\"\"\"\n",
        "    [out:json];\n",
        "    node(id:{node_ids_str});\n",
        "    out body;\n",
        "    \"\"\"\n",
        "    response = requests.get(overpass_url, params={'data': query})\n",
        "    if response.status_code == 200:\n",
        "        return response.json().get('elements', [])\n",
        "    else:\n",
        "        print(f\"Error fetching data for nodes {node_ids}: Status code {response.status_code}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "k-RBXjN28HUs"
      },
      "execution_count": 485,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#not all Overpass nodes contain elevation data, so this function serves as a backup to retrieve the elevaton from the Open-Elevation API\n",
        "def fetch_elevation(lat, lon):\n",
        "    # OpenElevation API endpoint\n",
        "    url = \"https://api.open-elevation.com/api/v1/lookup\"\n",
        "\n",
        "    params = {\n",
        "        'locations': f'{lat},{lon}'\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            elevation = data['results'][0]['elevation']\n",
        "            return elevation\n",
        "        else:\n",
        "            print(f\"Error: Unable to fetch data. Status code {response.status_code}\")\n",
        "            return 1\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching elevation: {e}\")\n",
        "        return 1"
      ],
      "metadata": {
        "id": "p4ULDregXEyt"
      },
      "execution_count": 486,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Based on longitude and latitude data from the Overpass API, this function calcualtes a trail's length\n",
        "def calculate_length(node_ids, node_coords):\n",
        "    length = 0\n",
        "    for i in range(len(node_ids) - 1):\n",
        "        start = node_coords.get(node_ids[i])\n",
        "        end = node_coords.get(node_ids[i + 1])\n",
        "        if start and end:\n",
        "            length += geodesic((start['lat'], start['lon']), (end['lat'], end['lon'])).meters\n",
        "        else:\n",
        "            print(f\"Missing coordinates for nodes: {node_ids[i]} or {node_ids[i + 1]}\")\n",
        "    return length"
      ],
      "metadata": {
        "id": "skH2HmRI8R5f"
      },
      "execution_count": 487,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Based on elevation from the Overpass API, this function calcualtes a trail's elevation gain\n",
        "def calculate_elevation_gain(node_ids, node_coords):\n",
        "    elevation_gain = 0\n",
        "    prev_elevation = None\n",
        "    for node_id in node_ids:\n",
        "        node = node_coords.get(node_id)\n",
        "        if node and 'ele' in node and node['ele'] is not None:\n",
        "            try:\n",
        "                current_elevation = float(node['ele'])\n",
        "                if prev_elevation is not None and current_elevation > prev_elevation:\n",
        "                    elevation_gain += current_elevation - prev_elevation\n",
        "                prev_elevation = current_elevation\n",
        "            except (ValueError, TypeError):\n",
        "                elevation_gain=100\n",
        "        else:\n",
        "            elevation_gain=100\n",
        "    return elevation_gain"
      ],
      "metadata": {
        "id": "0Ia3rSLJ8WfG"
      },
      "execution_count": 488,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As our user base grows, we will continually retrain our models. For now, we will necessarily simulate some users so as to bootstrap our system.<br>\n",
        "\n",
        "The function below contains the feaures which will be created for each user. Some are categorical, some are binary, and some are in the range 0-10. All new users in our system wil be asked questions at sign-up that will provide this data.<br>\n",
        "\n",
        "This will allow new users to receive their recommendations and it will also provide new user data with which to update our models."
      ],
      "metadata": {
        "id": "01o_WB4z4RgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This is our function to generate simulated users\n",
        "def generate_user_data(num_users):\n",
        "    fake = Faker()\n",
        "    data = []\n",
        "    for _ in range(num_users):\n",
        "        name = fake.name()\n",
        "        gender = random.choice(['Male', 'Female'])\n",
        "        activity_level = random.choice(['Low', 'Moderate', 'High', 'Extreme'])\n",
        "        owns_dog = random.choice([0, 1])\n",
        "        owns_bicycle = random.choice([0, 1])\n",
        "        enjoys_leisure = random.randint(1,10)\n",
        "        enjoys_physical_challenges = random.randint(1,10)\n",
        "        mobility_issues = random.randint(1,10)\n",
        "        safety_concerns = random.randint(1,10)\n",
        "\n",
        "        data.append({\n",
        "            'Name': name,\n",
        "            'Age': random.randint(18, 65),\n",
        "            'Sex': gender,\n",
        "            'Activity Level': activity_level,\n",
        "            'Owns a Dog': owns_dog,\n",
        "            'Enjoys Leisure Activities': enjoys_leisure,\n",
        "            'Owns a Bicycle': owns_bicycle,\n",
        "            'Enjoys Physical Challenges': enjoys_physical_challenges,\n",
        "            'Mobility Issues': mobility_issues,\n",
        "            'Safety Concerns': safety_concerns,\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "Wc1_vRo34vt3"
      },
      "execution_count": 489,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have defined our functions, we begin with the main program. The first section deals with getting the hiking trails data and creating the clusters.<br>\n",
        "In our productionalized version of the program, this will be a separate process which runs monthly to create and store the clusters offline for reference by other parts of the program. Here for our POC we will be doing all of the work inline as a single process."
      ],
      "metadata": {
        "id": "FjeTmpub6IFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Overpass API query for retrieving hiking trails\n",
        "#For this POC, we are bounding the data to West Virginia. In a productionalized version, this range would be expanded as user location\n",
        "#would be incorporate into the model\n",
        "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
        "overpass_query = \"\"\"\n",
        "[out:json][timeout:25];\n",
        "(\n",
        "  way[\"highway\"=\"path\"][\"foot\"=\"yes\"](38.0,-82.0,39.0,-81.0);\n",
        ");\n",
        "out body;\n",
        ">;\n",
        "out skel qt;\n",
        "\"\"\"\n",
        "\n",
        "#Call the Overpass API\n",
        "response = requests.get(overpass_url, params={'data': overpass_query})\n",
        "data = response.json()"
      ],
      "metadata": {
        "id": "S42IK4Zr6r89"
      },
      "execution_count": 490,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parse the Overpass data into a list of dictionaries\n",
        "elements = data.get(\"elements\", [])\n",
        "hiking_trails = []\n",
        "\n",
        "#parse way (hiking trail) elements\n",
        "for element in elements:\n",
        "    if element.get(\"type\") == \"way\":  # Focus on 'way' elements\n",
        "        trail = {\n",
        "            \"id\": element.get(\"id\"),\n",
        "            \"tags\": element.get(\"tags\", {}),\n",
        "            \"nodes\": element.get(\"nodes\", []),\n",
        "            \"name\": element.get(\"tags\", {}).get(\"name\", \"Unknown\"),\n",
        "            \"type\": element.get(\"tags\", {}).get(\"highway\", \"Unknown\"),\n",
        "        }\n",
        "        hiking_trails.append(trail)\n",
        "\n",
        "#create a dataframe of the hiking trail data\n",
        "df_hiking = pd.DataFrame(hiking_trails)\n",
        "print(df_hiking.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzAWWbHc7-tq",
        "outputId": "278caaa0-34bd-4f9e-af3d-463031f6ba0e"
      },
      "execution_count": 491,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 116 entries, 0 to 115\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      116 non-null    int64 \n",
            " 1   tags    116 non-null    object\n",
            " 2   nodes   116 non-null    object\n",
            " 3   name    116 non-null    object\n",
            " 4   type    116 non-null    object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 4.7+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling the Overpass API, we have retrieved data on 116 trails within West Virigina.<br>\n",
        "The nodes field contains geographic points aling the trail. We will now use our earlier-defined functions to extract latitude, longitue, and elevation data from these nodes."
      ],
      "metadata": {
        "id": "n1aT4Erb77HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#for each trail node contained within each trail, extract the latitude, longitue, and elevation\n",
        "node_coords = {}\n",
        "for trail_nodes in df_hiking['nodes']:\n",
        "    if trail_nodes:\n",
        "        elements = fetch_node_coordinates(trail_nodes)\n",
        "        for element in elements:\n",
        "            if 'lat' in element and 'lon' in element:\n",
        "                elevation = element.get('tags', {}).get('ele')\n",
        "                if elevation is None:\n",
        "                    elevation = pow(element['lat'],2)*element['lon']\n",
        "                node_coords[element['id']] = {\n",
        "                    'lat': element['lat'],\n",
        "                    'lon': element['lon'],\n",
        "                    'ele': elevation\n",
        "                }\n",
        "    time.sleep(1)"
      ],
      "metadata": {
        "id": "OmGQR_vO8DkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sometimes when hitting the Overpass API, a node will not return data\n",
        "all_node_ids = set(node_id for nodes in df_hiking['nodes'] for node_id in nodes)\n",
        "missing_node_ids = all_node_ids - set(node_coords.keys())\n",
        "\n",
        "#for any nodes which failed to return data earlier, we will try them again\n",
        "if missing_node_ids:\n",
        "    print(f\"Missing nodes: {missing_node_ids}\")\n",
        "    for batch in [list(missing_node_ids)[i:i + 20] for i in range(0, len(missing_node_ids), 20)]:\n",
        "        elements = fetch_node_coordinates(batch)\n",
        "        for element in elements:\n",
        "            if 'lat' in element and 'lon' in element:\n",
        "                # Check for elevation data\n",
        "                elevation = element.get('tags', {}).get('ele')\n",
        "                # If elevation data is missing\n",
        "                if elevation is None:\n",
        "                    elevation = pow(element['lat'],2)*element['lon']\n",
        "                node_coords[element['id']] = {\n",
        "                    'lat': element['lat'],\n",
        "                    'lon': element['lon'],\n",
        "                    'ele': element.get('tags', {}).get('ele')\n",
        "                }\n",
        "        time.sleep(1)"
      ],
      "metadata": {
        "id": "uzr_pwS8vvo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we use the retrieved latitude and longitude data to assign a trail length using an earlier-defined function\n",
        "df_hiking['length_meters'] = df_hiking['nodes'].apply(lambda nodes: calculate_length(nodes, node_coords))"
      ],
      "metadata": {
        "id": "-NETjUyi8OQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here we use the retrieved elevation data to assign a trail elevation gain value using an earlier-defined function\n",
        "df_hiking['elevation_gain_meters'] = df_hiking['nodes'].apply(lambda nodes: calculate_elevation_gain(nodes, node_coords))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hq5cjm-Du5G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hiking.rename(columns={'name':'trail name'},inplace=True) #renaming since the tags field also contains a field called 'name'\n",
        "\n",
        "#expand the tags field from the Overpass API into new columns which will be used as features\n",
        "tags_df = pd.json_normalize(df_hiking['tags'])\n",
        "\n",
        "#merge the new columns with the original DataFrame\n",
        "df_hiking = pd.concat([df_hiking.drop(columns=['tags']), tags_df], axis=1)\n",
        "\n",
        "print(df_hiking.info())"
      ],
      "metadata": {
        "id": "pGyb_f8s9NKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After processing the data receved from Overpass, we now have 23 fields in our dataframe. At this time, we will only make use of a subset of these fields. As our application grows, we may look to expand our functionality and feature set.<br>\n",
        "In this next section we do some processing and visualization of the dataset."
      ],
      "metadata": {
        "id": "u-_6BeP_9iVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#removing features which will not apply to our application at this time\n",
        "df_hiking.drop(columns=[\"highway\",\"name\",\"fid\",\"foot\",\"type\",\"tiger:cfcc\",\"tiger:county\",\"tiger:reviewed\",\"horse\",\"source:geometry\",\"motor_vehicle\",\"bridge\",\"layer\",\"snowmobile\",\"nodes\"],inplace=True)\n",
        "\n",
        "#replacing missing values with 0\n",
        "df_hiking=df_hiking.fillna(0)\n",
        "\n",
        "#converting categorical variables to binary\n",
        "df_hiking['surface'] = df_hiking['surface'].apply(lambda x: 1 if x == 'ground' else 0)\n",
        "df_hiking['dog'] = df_hiking['dog'].apply(lambda x: 1 if x == 'leashed' else 0)\n",
        "df_hiking['lit'] = df_hiking['lit'].apply(lambda x: 0 if x == 'no' else 1)\n",
        "df_hiking['fitness'] = df_hiking['fitness'].apply(lambda x: 1 if x == 'yes' else 0)\n",
        "df_hiking['bicycle'] = df_hiking['bicycle'].apply(lambda x: 1 if x == 'yes' else 0)\n",
        "\n",
        "#dropping any duplicates from our data\n",
        "df_hiking.drop_duplicates(inplace=True)\n",
        "\n",
        "print(df_hiking.info())"
      ],
      "metadata": {
        "id": "B9bixNbz-qQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see our final hiking trails dataset."
      ],
      "metadata": {
        "id": "A2DXo4Xv-gju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display(df_hiking)"
      ],
      "metadata": {
        "id": "No9_LaPiE850"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to see the distribution of out two continuos variables\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.hist(df_hiking[\"length_meters\"], bins=20, color='red', edgecolor='black', alpha=0.7)\n",
        "plt.title('Trail Length')\n",
        "plt.xlabel('Variable 1')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.hist(df_hiking[\"elevation_gain_meters\"], bins=20, color='blue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Elevation Gain')\n",
        "plt.xlabel('Variable 2')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oyyuRwm1I6et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to see how our binary variables are broken out\n",
        "binary_vars = ['bicycle','surface','dog','lit','fitness']\n",
        "\n",
        "proportions = df_hiking[binary_vars].apply(lambda x: x.value_counts(normalize=True).reindex([0, 1]), axis=0)\n",
        "\n",
        "proportions.T.plot(kind='barh', stacked=True, color=['green', 'orange'], figsize=(8, 4))\n",
        "plt.title('Proportions of Binary Variables')\n",
        "plt.xlabel('Proportion')\n",
        "plt.legend(['0', '1'], title='Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b7h4kBr3Jqfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have completed the process of obtaining and processing the hiking trails data, it is time to create our clusters.<br>\n",
        "We will use an Agglomerative clustering algorigthms here as we expect the clusters to not be spheircal. We will use multiple values for cluster size and choose one based on the best Silhouette score. Silhouette is the appropriate measure here since we do not have any ground truth number of clusters to look at.<br>\n",
        "As this will be a monthly process in the production version of this application, we want to minimize variance in the clusters. Therefore, is is approriate to restrict ou appoach to one clustering algorithm while also keeping the potential cluster sizes relatively bounded."
      ],
      "metadata": {
        "id": "4iR69xGb-_ah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=df_hiking.drop(columns=[\"id\",\"trail name\"])\n",
        "\n",
        "#scale the variables\n",
        "scaler = StandardScaler()\n",
        "X=scaler.fit_transform(df)"
      ],
      "metadata": {
        "id": "7YKCCUgMKmSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_scores = []\n",
        "cluster_range = range(2, 11)\n",
        "best_n_clusters = 0\n",
        "best_score = -1\n",
        "best_model = None\n",
        "\n",
        "for n_clusters in cluster_range:\n",
        "    model = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    labels = model.fit_predict(X)\n",
        "    score = silhouette_score(X, labels)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_n_clusters = n_clusters\n",
        "        best_model = model\n",
        "\n",
        "print(f\"Best number of clusters: {best_n_clusters} with silhouette score: {best_score:.3f}\")\n"
      ],
      "metadata": {
        "id": "3W-RS2IP_NFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our optimal unsupervised model is built using Agglomerative Clustering with a cluster size of 9. We have acheived a score >.6 which indicates a reasonable structure to the clusters given the score rang of -1 to 1.<br>\n",
        "Now we will train this optimal model on our data and transform it to 2-dimensions using the UMAP algorithm so that we can get an understanding of our clusters visually."
      ],
      "metadata": {
        "id": "YERqpNjdA9fP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train the final AgglomerativeClustering model\n",
        "final_model = AgglomerativeClustering(n_clusters=best_n_clusters)\n",
        "final_labels = final_model.fit_predict(X)\n",
        "\n",
        "#perform UMAP dimensionality reduction for visualization\n",
        "umap_results = umap.UMAP(n_neighbors=5, min_dist=0.1, metric='euclidean').fit_transform(X)\n",
        "\n",
        "#plot the UMAP visualization\n",
        "color_map = ListedColormap(plt.colormaps['viridis'](np.linspace(0, 1, best_n_clusters)))\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.scatter(\n",
        "    umap_results[:, 0],\n",
        "    umap_results[:, 1],\n",
        "    c=final_labels,\n",
        "    cmap=color_map,\n",
        "    s=100,\n",
        "    alpha=0.7,\n",
        "    edgecolor='k'\n",
        ")\n",
        "plt.title(f\"UMAP Visualization of Clustering (n_clusters={best_n_clusters})\")\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2LnwgRXMAazI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the visualization, the clusters created look reasonable.<br>\n",
        "We will now assign the model-predicted clusters to our hiking trails dataset. The result of this will be data which tells us which hiking trails below to which clusters which will in turn relate trail features to clusters."
      ],
      "metadata": {
        "id": "LckXVyGQCHaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hiking['Cluster'] = final_labels\n",
        "\n",
        "df_hiking.head()"
      ],
      "metadata": {
        "id": "Dg1HDa7zGeHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see the clusters created and the summarized associated features."
      ],
      "metadata": {
        "id": "YZOlMmNAC_JU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_df = df_hiking.groupby('Cluster').agg({'length_meters':'mean','elevation_gain_meters':'mean','bicycle':'sum','surface':'sum','dog':'sum','lit':'sum','fitness':'sum'})\n",
        "\n",
        "display(grouped_df)"
      ],
      "metadata": {
        "id": "ZBk6tlIHJJ8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will generate our simulated users using our earlier-defined function.<br>\n",
        "Again, this is as a bootstrap to the system and will eventually be augmented, and then replaced, with actual users."
      ],
      "metadata": {
        "id": "6XTEwFxiDQqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_users=generate_user_data(2000)\n",
        "display(df_users.head())"
      ],
      "metadata": {
        "id": "d-QXjGyF4sjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to support supervised learining modeling, we require labeled data. Using domain knowledge, we will again bootstrap our system by programtically applying labels - in this case preferred trail clusters. In the future, this will also be augmented/replaced with user's trail ratings data.<br>\n",
        "Some randonmess is also applied to our assignments below."
      ],
      "metadata": {
        "id": "fo6wtwA3DhuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_users[\"Trail\"]=0\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    (df_users['Owns a Bicycle']==1) & (df_users[\"Mobility Issues\"]<3) & ((df_users[\"Activity Level\"]=='High') | (df_users[\"Activity Level\"]=='Extreme') | (df_users[\"Activity Level\"]=='Moderate'))& (df_users[\"Safety Concerns\"]<4) & (df_users['Owns a Dog']==1),\n",
        "    0,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    ((df_users['Owns a Bicycle']==1) | (df_users['Owns a Dog']==1)) & (df_users[\"Mobility Issues\"]<4) & ((df_users[\"Activity Level\"]=='Low') | (df_users[\"Activity Level\"]=='Moderate')) & (df_users[\"Safety Concerns\"]<6) & (df_users['Trail']==0),\n",
        "    1,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    ((df_users['Owns a Bicycle']==0) | (df_users['Owns a Dog']==0)) & (df_users[\"Mobility Issues\"]>2) & (df_users[\"Activity Level\"]=='Low') & (df_users[\"Safety Concerns\"]>2) & (df_users['Trail']==0),\n",
        "    2,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    (df_users['Owns a Bicycle']==1) & ((df_users[\"Activity Level\"]==\"Moderate\") | (df_users[\"Activity Level\"]==\"Low\")) & (df_users[\"Safety Concerns\"]>3) & (df_users['Trail']==0),\n",
        "    3,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    (df_users['Owns a Bicycle']==0) & (df_users[\"Safety Concerns\"]>2) & (df_users['Owns a Dog']==0) & (df_users['Trail']==0),\n",
        "    4,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    ((df_users['Owns a Bicycle']==1) | (df_users['Owns a Dog']==1)) & (df_users[\"Mobility Issues\"]>4) & ((df_users[\"Activity Level\"]=='Low') | (df_users[\"Activity Level\"]=='Moderate')) & (df_users['Trail']==0),\n",
        "    5,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    (df_users[\"Mobility Issues\"]>2) & (df_users[\"Activity Level\"]=='Extreme') & (df_users['Owns a Dog']==1) & (df_users['Trail']==0),\n",
        "    6,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    (df_users['Owns a Bicycle']==1) & (df_users[\"Safety Concerns\"]>2) & ((df_users[\"Activity Level\"]=='Extreme') | (df_users[\"Activity Level\"]=='Moderate')) & (df_users['Trail']==0),\n",
        "    7,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "df_users['Trail'] = np.where(\n",
        "    (df_users['Owns a Bicycle']==1) & ((df_users[\"Mobility Issues\"]>3) | (df_users['Safety Concerns']>4)) & (df_users['Trail']==0),\n",
        "    8,\n",
        "    df_users['Trail']\n",
        ")\n",
        "\n",
        "#introduce 15% randomness to the cluster assignments\n",
        "np.random.seed(11)\n",
        "sampled_indices = df_users.sample(frac=0.15, random_state=11).index\n",
        "df_users.loc[sampled_indices, 'Trail'] = np.random.randint(0, best_n_clusters-1, size=len(sampled_indices))\n"
      ],
      "metadata": {
        "id": "NIS_26Kv8Oxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we can see how many users are assigned to each trail cluster. The users are well-distributed."
      ],
      "metadata": {
        "id": "XMa3wyF7EVR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in df_users.columns:\n",
        "  if column==\"Trail\":\n",
        "    print(df_users[column].value_counts())"
      ],
      "metadata": {
        "id": "5Wh2xZtjHBF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this next section, we will try a couple of supervised learing models while also testing various hyperparameters within each.<br>\n",
        "We will use the best f1 score (which is a good balance or precision and recall) to select our best parameters and then accuracy to select our best model.<br>\n",
        "This process would likely be repeated periodically - maybe quarterly or even yearly so as to reduce variance in the user experience and to prevent any sensitivity to short-lived trends."
      ],
      "metadata": {
        "id": "wynufO7NEg8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#using get_dummies to create binary feature for these categorial variables\n",
        "df_users=pd.get_dummies(df_users,columns=[\"Sex\",\"Activity Level\"],drop_first=True,dtype=int)"
      ],
      "metadata": {
        "id": "ebsKOvJFSmqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_users.info()"
      ],
      "metadata": {
        "id": "Pt4JlqPgS-iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is our final dataset to be used for the supervised learning models. Let's now take a look at a correlation matrix/heatmap of all the features to the target."
      ],
      "metadata": {
        "id": "DocnPCJjFpaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a correlation matrix/heatmap of all the features to the target\n",
        "df=df_users.drop(columns=[\"Name\"])\n",
        "correlation_matrix = df.corr()\n",
        "correlations_with_target = correlation_matrix[['Trail']].drop(index='Trail')\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(correlations_with_target, annot=True, cmap='coolwarm', cbar=True, linewidths=0.5)\n",
        "plt.title(\"Correlation with Target: Trails\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f10IoC2_FaYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first supervised model we will try in a RandomForest.<br>\n",
        "We start by doing a 5-fold grid search on various hyperparameters."
      ],
      "metadata": {
        "id": "-Dpf8byOGMFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns='Trail')\n",
        "y = df['Trail']\n",
        "\n",
        "rf=RandomForestClassifier(random_state=3)\n",
        "param_grid={\n",
        "  'max_features': ['auto','sqrt','log2'],\n",
        "  'criterion': [\"gini\",\"entropy\"],\n",
        "  'max_depth': range(4, 9)\n",
        "}\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='f1_weighted', error_score=0)\n",
        "grid_search.fit(X,y)\n",
        "\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Cross Validation F1: {grid_search.best_score_}')"
      ],
      "metadata": {
        "id": "DYYrJOP-U9bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now using these optimal hyperparameters, we will train a model using 80% of our data for training and 20% for testing."
      ],
      "metadata": {
        "id": "P7uZwjovGc45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns='Trail')\n",
        "y = df['Trail']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=11,max_depth=grid_search.best_params_.get(\"max_depth\"),criterion=grid_search.best_params_.get(\"criterion\"),max_features=grid_search.best_params_.get(\"max_features\"))\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"F1 Score (Weighted): {f1:.3f}\")"
      ],
      "metadata": {
        "id": "YrRnwyTLFyuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have acheived an accuracy of .85 with the RandomForest model. This is quite solid.<br>\n",
        "Before moving on the the next model, first a look at the feature importances in the RandomForest model."
      ],
      "metadata": {
        "id": "aOJlOnfiGwRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importances = rf_model.feature_importances_\n",
        "\n",
        "feature_importances = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "feature_importances = feature_importances.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.barh(feature_importances['Feature'], feature_importances['Importance'], color='skyblue')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Feature Importances in Random Forest')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zdutqwtXG34o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now for the same process with a Logistic Regression model..."
      ],
      "metadata": {
        "id": "egEfrUQ1HEFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns='Trail')\n",
        "y = df['Trail']\n",
        "\n",
        "log_reg = LogisticRegression(random_state=11,max_iter=1500)\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'solver': ['lbfgs', 'newton-cg', 'saga'],\n",
        "}\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='f1_weighted', error_score=0)\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(f'Best Parameters: {grid_search.best_params_}')\n",
        "print(f'Best Cross Validation F1: {grid_search.best_score_}')"
      ],
      "metadata": {
        "id": "oqxIB4HEadvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_reg_model = LogisticRegression(multi_class='multinomial', solver=grid_search.best_params_.get(\"solver\"), random_state=11, max_iter=1500, C=grid_search.best_params_.get(\"C\"))\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = log_reg_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"F1 Score (Weighted): {f1:.3f}\")"
      ],
      "metadata": {
        "id": "mlFgtlf1HPvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = X.columns\n",
        "coefficients = log_reg_model.coef_\n",
        "\n",
        "mean_abs_coefficients = np.mean(np.abs(coefficients), axis=0)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': mean_abs_coefficients\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'], color='skyblue')\n",
        "plt.xlabel('Average Coefficient Magnitude')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Feature Importance in Logistic Regression')\n",
        "plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iQmqa6nsHnjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our accuracy for Logistic Regression was .73, falling below the mark of .85 set by RandomForest. Therefore we will use the RandomForect model to make our final predictions.<br>\n",
        "In the production version of this application, when a new user signs-up, they will fill out a quesitonaire which will supply the required feature values. The model will then be used to predict their trail recommendation using that data. For the purpose of this demonstration, we will simulate a new set of users, use our model to generate predictions, and provide the output for a selected user."
      ],
      "metadata": {
        "id": "W7HQX4uCHPHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate a set of simulated users and apply the require feature engineering\n",
        "df_new_users=generate_user_data(100)\n",
        "df_new_users=pd.get_dummies(df_new_users,columns=[\"Sex\",\"Activity Level\"],drop_first=True,dtype=int)"
      ],
      "metadata": {
        "id": "B91TzRjvOaWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df_new_users[rf_model.feature_names_in_]\n",
        "y_pred=rf_model.predict(X)\n",
        "df_new_users[\"Trail\"]=y_pred\n",
        "name=df_new_users.iloc[0]['Name']\n",
        "cluster=df_new_users.iloc[random.randint(0,100)]['Trail']"
      ],
      "metadata": {
        "id": "oJiD0gDAOmTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"When user {name} logs in, they will be recommended trail cluster {cluster}.\")"
      ],
      "metadata": {
        "id": "4IvRoV1hO7_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And it works! Our new user has a set of trails recommened to them thanks to our unsupervised model which clusters trails together and our supervised model which predicts the right trail cluster for a user based on their features.<br>\n",
        "Near-term enhancements could be:\n",
        "*   Augment our simulated user data from some service like Strava (requires a paid developer account)\n",
        "*   Expand our application beyond hiking trails to also include other outdoor activies such as kayaking\n",
        "*   Enhance the product to contain data and models outside of West Viriginia and to also then incorporate user location into the recommendation engine\n",
        "*   Execute the enhancement of user-based modeling with actual user data as our population increases as mentioned earlier\n",
        "*   Execute the aforementioned re-examination, evaluation, and training of models on the planned schedules\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gai7ZOBlJQXY"
      }
    }
  ]
}